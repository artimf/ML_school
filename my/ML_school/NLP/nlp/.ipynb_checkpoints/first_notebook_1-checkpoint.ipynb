{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1. Классификация обращений пользователей SAP в поддержку"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Постановка задачи и данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напомним постановку задачи, которую предстоит решить в первом практическом задании. По тексту обращения необходимо определить пользовательский интент и классифицировать его в один из существующих классов-интентов:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/1_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим таблицу с данными. Для считвания данных и их хранения используется библиотека $pandas$, которая предоставляет удобный интерфейс для работы с данными:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>После оплаты , выполнения ЗнЗ, не получается и...</td>\n",
       "      <td>1-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Вопрос/проблема: Не дает исполнить ЗНС</td>\n",
       "      <td>1-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Вопрос/проблема: Не могу исполнить знз 2076251...</td>\n",
       "      <td>1-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Вопрос/проблема: Дорбрый день! Пытаюсь провест...</td>\n",
       "      <td>1-71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>При попытке подписания документа внутри банка ...</td>\n",
       "      <td>1-71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text class\n",
       "0  После оплаты , выполнения ЗнЗ, не получается и...   1-1\n",
       "1            Вопрос/проблема: Не дает исполнить ЗНС    1-1\n",
       "2  Вопрос/проблема: Не могу исполнить знз 2076251...  1-22\n",
       "3  Вопрос/проблема: Дорбрый день! Пытаюсь провест...  1-71\n",
       "4  При попытке подписания документа внутри банка ...  1-71"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "df = pd.read_excel(\"last_zno_without_ner.xlsx\", index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таблица содержит два поля:\n",
    "- **text** - Текст обращения пользователя.\n",
    "- **class** - Класс проблемы, содержащейся в обращении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, тексты обращений не могут быть использованы как входные данные моделей машинного обучения. Чтобы преобразовать их в необходимый формат, мы должны пройти через три этапа:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/1_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Токенизация** - это процесс выделеения токенов из текста, где в качестве токенов могут выступать символы, слова, словосочетания или предложения. Выше изображена токенизация текста по словам и именно ее мы будем далее использовать.\n",
    "- После того, как проведена токенизация для каждого обращения, мы получаем набор из большого количества слов. Некоторые из этих слов представляют собой различные формы одного и того же слова. От всех таких слов хочется оставить лишь одного представителя, что происходит при **предобработке** токенов.\n",
    "- После того, как мы сократили общее число слов, нам необходимо выбрать наиболее информативные слова и сделать их **признаками** текста. Затем каждому обращению мы можем сопоставить вектор, который и будут использовать модели машинного обучения:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/1_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На $i$-ой компоненте результирующего вектора стоит единица, если $i$-ое слово-признак присутствует в тексте и 0 в обратном случае. Такой вектор называется **признаковым описанием** обращения. Вместе с индикаторными признаковыми описаниями, построение которых мы только что рассмотрели, мы можем использовать частотные: в случае присутсвия слова-признака в тексте мы записываем в соответствующую компоненту вектора не 1, а частоту слова-признака в тексте. Далее в практическом задании мы будем использовать именно частотные признаковые описания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь более подробно поговорим о каждом этапе преобразования сырого текста обращения пользователя в вектор признакового описания, последовательно применяя описанные этапы к нашим данным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прежде чем переходить к програмной реализации, опишем несколько основных приемых приемов, которые мы будем использовать в дальнейшем при обработке данных:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Чтобы применить преобразование к каждому элементу типа $Series$ (такой тип имеют столбцы матрицы  $df$), можно использовать метод $apply()$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    1\n",
       "3    0\n",
       "4    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Series([1, 2, 3, 4, 5])\n",
    "function = lambda x : x % 2\n",
    "a.apply(function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Чтобы применить преобразование к каждому элементу типа $list$, можно использовать списковое включение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3, 4, 5]\n",
    "[x + 1 for x in a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- В списковом включении также можно использовать условие:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3, 4, 5]\n",
    "[x + 1 for x in a if x > 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/1_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как оговаривалось ранее, мы выбрали в качестве токенов слова и теперь необходимо каждое обращение представить набором содержащихся в нем слов. При этом нет необходимости в прмяом смысле сопоставлять обращению список слов: достаточно выписать все слова, которые встречаются в тексте через пробел и каждому обращению сопоставить такую строку. Тогда мы сможем хранить токенизированные обращения в той же таблице $df$, где хранятся необработанные тексты обращений, всего лишь добавив в таблицу новый столбец."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, нам необходимо каждому обращению сопоставить строку из содержащихся в нем слов, выписанных через пробел.Такая задача сводится к удалению из исходного обращения:\n",
    "- Символов пунктуации\n",
    "- Отдельно стоящих чисел"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оговорка \"отдельно стоящие\" нужна для того, чтобы не удалить из текста, например,  токен \"ос1\", который содержит цифру, но, тем не менее, имеет важное значение при классификации. Подробнее о таких токенах мы поговорим на этапе осуществления замен в тексте."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Удаление знаков препинания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Произведем удаление символов пунктуации из текстов. Сделаем это с помощью метода $sub$ библиотеки $re$, который удаляет из строки все символы из заданного списка, а также с помощью метода $apply$, который мы обсуждали ранее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "deleted_symols = '[\\\\\\\\\\'[\\]!\"$%&()*+,-./:;<=>?@^_`{|}~«»\\n]'\n",
    "func = lambda text : re.sub(deleted_symols, ' ', text)\n",
    "df[\"without_punctuation_text\"] = df[\"text\"].apply(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что символы пунктуации были удалены:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>without_punctuation_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>После оплаты , выполнения ЗнЗ, не получается и...</td>\n",
       "      <td>1-1</td>\n",
       "      <td>После оплаты   выполнения ЗнЗ  не получается и...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Вопрос/проблема: Не дает исполнить ЗНС</td>\n",
       "      <td>1-1</td>\n",
       "      <td>Вопрос проблема  Не дает исполнить ЗНС</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Вопрос/проблема: Не могу исполнить знз 2076251...</td>\n",
       "      <td>1-22</td>\n",
       "      <td>Вопрос проблема  Не могу исполнить знз 2076251...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Вопрос/проблема: Дорбрый день! Пытаюсь провест...</td>\n",
       "      <td>1-71</td>\n",
       "      <td>Вопрос проблема  Дорбрый день  Пытаюсь провест...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>При попытке подписания документа внутри банка ...</td>\n",
       "      <td>1-71</td>\n",
       "      <td>При попытке подписания документа внутри банка ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text class  \\\n",
       "0  После оплаты , выполнения ЗнЗ, не получается и...   1-1   \n",
       "1            Вопрос/проблема: Не дает исполнить ЗНС    1-1   \n",
       "2  Вопрос/проблема: Не могу исполнить знз 2076251...  1-22   \n",
       "3  Вопрос/проблема: Дорбрый день! Пытаюсь провест...  1-71   \n",
       "4  При попытке подписания документа внутри банка ...  1-71   \n",
       "\n",
       "                            without_punctuation_text  \n",
       "0  После оплаты   выполнения ЗнЗ  не получается и...  \n",
       "1            Вопрос проблема  Не дает исполнить ЗНС   \n",
       "2  Вопрос проблема  Не могу исполнить знз 2076251...  \n",
       "3  Вопрос проблема  Дорбрый день  Пытаюсь провест...  \n",
       "4  При попытке подписания документа внутри банка ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Удаление отдельно стоящих чисел"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Упражнение 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание:** Удалите из текстов обращений отдельно стоящие числа по следующему алгоритму:\n",
    "- Разбейте текст обращения на слова с помощью функции $split()$\n",
    "- Используя списковое включение удалите из полученного списка числа (для того, чтобы проверить, является ли строка числом используйте метод $isdigit$)\n",
    "- Преобразуйте список в строку с помощью метода $join()$\n",
    "- Примените вышеописанные действия к каждому обращению таблицы $df$ с помощью функции $apply()$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ошибка номер', 'документ ос1', '12345t6789', '']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>without_punctuation_text</th>\n",
       "      <th>without_numbers_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>После оплаты , выполнения ЗнЗ, не получается и...</td>\n",
       "      <td>1-1</td>\n",
       "      <td>После оплаты   выполнения ЗнЗ  не получается и...</td>\n",
       "      <td>После оплаты выполнения ЗнЗ не получается испо...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Вопрос/проблема: Не дает исполнить ЗНС</td>\n",
       "      <td>1-1</td>\n",
       "      <td>Вопрос проблема  Не дает исполнить ЗНС</td>\n",
       "      <td>Вопрос проблема Не дает исполнить ЗНС</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Вопрос/проблема: Не могу исполнить знз 2076251...</td>\n",
       "      <td>1-22</td>\n",
       "      <td>Вопрос проблема  Не могу исполнить знз 2076251...</td>\n",
       "      <td>Вопрос проблема Не могу исполнить знз по догов...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Вопрос/проблема: Дорбрый день! Пытаюсь провест...</td>\n",
       "      <td>1-71</td>\n",
       "      <td>Вопрос проблема  Дорбрый день  Пытаюсь провест...</td>\n",
       "      <td>Вопрос проблема Дорбрый день Пытаюсь провести ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>При попытке подписания документа внутри банка ...</td>\n",
       "      <td>1-71</td>\n",
       "      <td>При попытке подписания документа внутри банка ...</td>\n",
       "      <td>При попытке подписания документа внутри банка ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text class  \\\n",
       "0  После оплаты , выполнения ЗнЗ, не получается и...   1-1   \n",
       "1            Вопрос/проблема: Не дает исполнить ЗНС    1-1   \n",
       "2  Вопрос/проблема: Не могу исполнить знз 2076251...  1-22   \n",
       "3  Вопрос/проблема: Дорбрый день! Пытаюсь провест...  1-71   \n",
       "4  При попытке подписания документа внутри банка ...  1-71   \n",
       "\n",
       "                            without_punctuation_text  \\\n",
       "0  После оплаты   выполнения ЗнЗ  не получается и...   \n",
       "1            Вопрос проблема  Не дает исполнить ЗНС    \n",
       "2  Вопрос проблема  Не могу исполнить знз 2076251...   \n",
       "3  Вопрос проблема  Дорбрый день  Пытаюсь провест...   \n",
       "4  При попытке подписания документа внутри банка ...   \n",
       "\n",
       "                                without_numbers_text  \n",
       "0  После оплаты выполнения ЗнЗ не получается испо...  \n",
       "1              Вопрос проблема Не дает исполнить ЗНС  \n",
       "2  Вопрос проблема Не могу исполнить знз по догов...  \n",
       "3  Вопрос проблема Дорбрый день Пытаюсь провести ...  \n",
       "4  При попытке подписания документа внутри банка ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Начало кода\n",
    "func = lambda text : \" \".join([i for i in text.split() if not i.isdigit()])\n",
    "\n",
    "sentence_list = [\"ошибка номер\", \"документ ос1\", \"12345t6789\", \"33\"]\n",
    "x=[func(sentence) for sentence in sentence_list]\n",
    "print(x) \n",
    "\n",
    "df['without_numbers_text'] = df[\"without_punctuation_text\"].apply(func)\n",
    "#Конец кода\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 23'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"  2 23 \".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проверка:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ к упражнению 1:\n",
      "ошибка номер документ ос1 12345t6789\n"
     ]
    }
   ],
   "source": [
    "sentence_list = [\"ошибка номер\", \"документ ос1\", \"12345t6789\", \"\"]\n",
    "union_res = \" \".join([func(sentence) for sentence in sentence_list]).strip()\n",
    "\n",
    "print(\"Ответ к упражнению 1:\")\n",
    "print(union_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На это токенизация обращений завершена. Мы можем удалить из таблицы $df$ столбцы, отвечающие промежуточным преобразованиям и оставить только последний вариант:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokenized_text\"] = df['without_numbers_text']\n",
    "df = df.drop([\"without_numbers_text\", \"without_punctuation_text\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>После оплаты , выполнения ЗнЗ, не получается и...</td>\n",
       "      <td>1-1</td>\n",
       "      <td>После оплаты выполнения ЗнЗ не получается испо...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Вопрос/проблема: Не дает исполнить ЗНС</td>\n",
       "      <td>1-1</td>\n",
       "      <td>Вопрос проблема Не дает исполнить ЗНС</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Вопрос/проблема: Не могу исполнить знз 2076251...</td>\n",
       "      <td>1-22</td>\n",
       "      <td>Вопрос проблема Не могу исполнить знз по догов...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Вопрос/проблема: Дорбрый день! Пытаюсь провест...</td>\n",
       "      <td>1-71</td>\n",
       "      <td>Вопрос проблема Дорбрый день Пытаюсь провести ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>При попытке подписания документа внутри банка ...</td>\n",
       "      <td>1-71</td>\n",
       "      <td>При попытке подписания документа внутри банка ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text class  \\\n",
       "0  После оплаты , выполнения ЗнЗ, не получается и...   1-1   \n",
       "1            Вопрос/проблема: Не дает исполнить ЗНС    1-1   \n",
       "2  Вопрос/проблема: Не могу исполнить знз 2076251...  1-22   \n",
       "3  Вопрос/проблема: Дорбрый день! Пытаюсь провест...  1-71   \n",
       "4  При попытке подписания документа внутри банка ...  1-71   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0  После оплаты выполнения ЗнЗ не получается испо...  \n",
       "1              Вопрос проблема Не дает исполнить ЗНС  \n",
       "2  Вопрос проблема Не могу исполнить знз по догов...  \n",
       "3  Вопрос проблема Дорбрый день Пытаюсь провести ...  \n",
       "4  При попытке подписания документа внутри банка ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/1_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После токенизации нам необходимо привести все слова обращений к некоторой нормальной форме. Такое преобразование включает в себя:\n",
    "- Приведение символов слов к нижнему регистру\n",
    "- Приведение слов к нормальной форме с помощью стемминга или лемматизации (будем использовать лемматизацию)\n",
    "- Осуществление списка заданных замен"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед началом преобразований посчитаем количество уникальных слов во всех обращениях:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34222"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words = len(set(\" \".join(df[\"tokenized_text\"]).split()))\n",
    "n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, насколько это число изменится после предобработки обращений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Приведение к нижнему регистру"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приведение всех обращений к нижнему регистру осуществляется с помощью строковго метода $lower$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lowercase_text\"] = df[\"tokenized_text\"].apply(lambda text : text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>lowercase_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>После оплаты , выполнения ЗнЗ, не получается и...</td>\n",
       "      <td>1-1</td>\n",
       "      <td>После оплаты выполнения ЗнЗ не получается испо...</td>\n",
       "      <td>после оплаты выполнения знз не получается испо...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Вопрос/проблема: Не дает исполнить ЗНС</td>\n",
       "      <td>1-1</td>\n",
       "      <td>Вопрос проблема Не дает исполнить ЗНС</td>\n",
       "      <td>вопрос проблема не дает исполнить знс</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Вопрос/проблема: Не могу исполнить знз 2076251...</td>\n",
       "      <td>1-22</td>\n",
       "      <td>Вопрос проблема Не могу исполнить знз по догов...</td>\n",
       "      <td>вопрос проблема не могу исполнить знз по догов...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Вопрос/проблема: Дорбрый день! Пытаюсь провест...</td>\n",
       "      <td>1-71</td>\n",
       "      <td>Вопрос проблема Дорбрый день Пытаюсь провести ...</td>\n",
       "      <td>вопрос проблема дорбрый день пытаюсь провести ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>При попытке подписания документа внутри банка ...</td>\n",
       "      <td>1-71</td>\n",
       "      <td>При попытке подписания документа внутри банка ...</td>\n",
       "      <td>при попытке подписания документа внутри банка ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text class  \\\n",
       "0  После оплаты , выполнения ЗнЗ, не получается и...   1-1   \n",
       "1            Вопрос/проблема: Не дает исполнить ЗНС    1-1   \n",
       "2  Вопрос/проблема: Не могу исполнить знз 2076251...  1-22   \n",
       "3  Вопрос/проблема: Дорбрый день! Пытаюсь провест...  1-71   \n",
       "4  При попытке подписания документа внутри банка ...  1-71   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  После оплаты выполнения ЗнЗ не получается испо...   \n",
       "1              Вопрос проблема Не дает исполнить ЗНС   \n",
       "2  Вопрос проблема Не могу исполнить знз по догов...   \n",
       "3  Вопрос проблема Дорбрый день Пытаюсь провести ...   \n",
       "4  При попытке подписания документа внутри банка ...   \n",
       "\n",
       "                                      lowercase_text  \n",
       "0  после оплаты выполнения знз не получается испо...  \n",
       "1              вопрос проблема не дает исполнить знс  \n",
       "2  вопрос проблема не могу исполнить знз по догов...  \n",
       "3  вопрос проблема дорбрый день пытаюсь провести ...  \n",
       "4  при попытке подписания документа внутри банка ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Приведение к нормальной форме"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После приведения слов обращений к нижнему регистру необходимо лемматизировать обращения. Это можно сделать двумя способами:\n",
    "- **Способ 1**\n",
    "    - Создать объект класса $MorphAnalyzer()$\n",
    "    - Представить обращение списком слов\n",
    "    - Каждое обращение из списка привести к нормальной форме методом $normal\\_form$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Способ 2**\n",
    "    - Создать объект класса $MorphAnalyzer()$\n",
    "    - Создать список всех слов во всех обращениях\n",
    "    - Найти уникальные слова (удобно представить список объектом $Series$ и использовать метод $unique()$)\n",
    "    - Из уникальных слов составить словарь, сопоставляющий нелемматизированным словам лемматизированные\n",
    "    - Преобразовать все слова во всех обращениях"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существенным преимуществом второго подхода является тот факт, что мы не применяем дорогую операцию преобразования слова к лемматизированной форме к одному и тому же слову несколько раз. К примеру, если некоторое слово встречается в текстах всех обращений 100 раз, (что и происходит для часто употребительных слов) то в первом подходе мы будем лемматизировать его так же 100 раз, а во втором подходе - всего лишь один. Почувствуем разницу в эффективности на практике:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализация первого способа выглядит следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy2\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
      "Collecting dawg-python>=0.7 (from pymorphy2)\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
      "Collecting docopt>=0.6 (from pymorphy2)\n",
      "  Downloading https://files.pythonhosted.org/packages/a2/55/8f8cab2afd404cf578136ef2cc5dfb50baa1761b68c9da1fb1e4eed343c9/docopt-0.6.2.tar.gz\n",
      "Collecting pymorphy2-dicts<3.0,>=2.4 (from pymorphy2)\n",
      "  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
      "Building wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13709 sha256=274753d176ca75af1762a2227d1efdc130c8766a5fbcbb6f380d6cb4723b8b93\n",
      "  Stored in directory: C:\\Users\\F4540~1\\AppData\\Local\\pip\\Cache\\wheels\\9b\\04\\dd\\7daf4150b6d9b12949298737de9431a324d4b797ffd63f526e\n",
      "Successfully built docopt\n",
      "Installing collected packages: dawg-python, docopt, pymorphy2-dicts, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "def to_lemmatize1(df):\n",
    "    lemmatizer = MorphAnalyzer()\n",
    "    lemm_func = lambda text: ' '.join([lemmatizer.normal_forms(word)[0] for word in text.split()])\n",
    "    df['lemmatized_text'] = df['lowercase_text'].apply(lemm_func)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим время работы первого варианта лемматизации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text class  \\\n",
      "0      После оплаты , выполнения ЗнЗ, не получается и...   1-1   \n",
      "1                Вопрос/проблема: Не дает исполнить ЗНС    1-1   \n",
      "2      Вопрос/проблема: Не могу исполнить знз 2076251...  1-22   \n",
      "3      Вопрос/проблема: Дорбрый день! Пытаюсь провест...  1-71   \n",
      "4      При попытке подписания документа внутри банка ...  1-71   \n",
      "...                                                  ...   ...   \n",
      "19307  Добрый день в SAP УВХД, необходимо предоставит...   6-1   \n",
      "19308  Добрый день! Ранее направлялся запрос № SD0075...  1-71   \n",
      "19309  Нет возможности предоставить роли сотруднику #...   111   \n",
      "19310  Добрый день! При закрытии исполненного договор...  1-72   \n",
      "19311  При исполнении ЗНР07210183 , ЗНР07210184, ЗНР0...   111   \n",
      "\n",
      "                                          tokenized_text  \\\n",
      "0      После оплаты выполнения ЗнЗ не получается испо...   \n",
      "1                  Вопрос проблема Не дает исполнить ЗНС   \n",
      "2      Вопрос проблема Не могу исполнить знз по догов...   \n",
      "3      Вопрос проблема Дорбрый день Пытаюсь провести ...   \n",
      "4      При попытке подписания документа внутри банка ...   \n",
      "...                                                  ...   \n",
      "19307  Добрый день в SAP УВХД необходимо предоставить...   \n",
      "19308  Добрый день Ранее направлялся запрос № SD00753...   \n",
      "19309  Нет возможности предоставить роли сотруднику #...   \n",
      "19310  Добрый день При закрытии исполненного договора...   \n",
      "19311  При исполнении ЗНР07210183 ЗНР07210184 ЗНР0721...   \n",
      "\n",
      "                                          lowercase_text  \\\n",
      "0      после оплаты выполнения знз не получается испо...   \n",
      "1                  вопрос проблема не дает исполнить знс   \n",
      "2      вопрос проблема не могу исполнить знз по догов...   \n",
      "3      вопрос проблема дорбрый день пытаюсь провести ...   \n",
      "4      при попытке подписания документа внутри банка ...   \n",
      "...                                                  ...   \n",
      "19307  добрый день в sap увхд необходимо предоставить...   \n",
      "19308  добрый день ранее направлялся запрос № sd00753...   \n",
      "19309  нет возможности предоставить роли сотруднику #...   \n",
      "19310  добрый день при закрытии исполненного договора...   \n",
      "19311  при исполнении знр07210183 знр07210184 знр0721...   \n",
      "\n",
      "                                         lemmatized_text  \n",
      "0      после оплата выполнение знз не получаться испо...  \n",
      "1                вопрос проблема не давать исполнить знс  \n",
      "2      вопрос проблема не мочь исполнить знз по догов...  \n",
      "3      вопрос проблема дорбрый день пытаться провести...  \n",
      "4      при попытка подписание документ внутри банка п...  \n",
      "...                                                  ...  \n",
      "19307  добрый день в sap увхд необходимый предоставит...  \n",
      "19308  добрый день ранее направляться запрос № sd0075...  \n",
      "19309  нет возможность предоставить роль сотрудник #ф...  \n",
      "19310  добрый день при закрытие исполнить договор воз...  \n",
      "19311  при исполнение знр07210183 знр07210184 знр0721...  \n",
      "\n",
      "[19312 rows x 5 columns]\n",
      "Wall time: 2min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = to_lemmatize1(df)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Упражнение 2. \n",
    "**Задание:** Реализуйте второй вариант приведения к нормальной форме согласно описанному выше алгоритму:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lemmatized_text\"] = df[\"lowercase_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "all_word_str = \" \".join(df[\"lowercase_text\"])\n",
    "all_word_list = all_word_str.split()\n",
    "#Начало кода\n",
    "#all_unique_word =None\n",
    "all_unique_word = pd.Series(all_word_list).unique()\n",
    "print(all_unique_word[:10])\n",
    "#Конец кода\n",
    "lemmatizer = MorphAnalyzer()\n",
    "xx= [lemmatizer.normal_forms('получается')]\n",
    "print(''.join(xx[0]))\n",
    "\"\"\"\n",
    "def to_lemmatize2(df):\n",
    "    all_word_str = \" \".join(df[\"lowercase_text\"])\n",
    "    all_word_list = all_word_str.split()\n",
    "    #Начало кода\n",
    "    all_unique_word = pd.Series(all_word_list).unique()\n",
    "    print(all_unique_word)\n",
    "    #Конец кода\n",
    "    lemmatized_word_dict = {}\n",
    "    lemmatizer = MorphAnalyzer()\n",
    "    for word in all_unique_word:\n",
    "        #Начало кода\n",
    "        #print(''.join([lemmatizer.normal_forms(word)][0]) )\n",
    "        lemmatized_word_dict[word] = ''.join([lemmatizer.normal_forms(word)][0])\n",
    "        #Конец кода\n",
    "    lemm_func = lambda text: ' '.join([lemmatized_word_dict[word] for word in text.split()])\n",
    "    df['lemmatized_text'] = df['lowercase_text'].apply(lemm_func)\n",
    "    return df, all_unique_word\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проверка:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['у' 'меня' 'возникла' 'какая-то' 'ошибка' 'ничего' 'не' 'работает'\n",
      " 'почему' 'именно' 'проблемы']\n",
      "Ответ к упражнению 2:\n",
      "у я возникнуть какой-то ошибка ничтоничего не работать почему именно у я проблема\n"
     ]
    }
   ],
   "source": [
    "check_df = DataFrame({\"lowercase_text\" : [\"у меня возникла какая-то ошибка\", \n",
    "                                          \"ничего не работает\", \n",
    "                                          \"почему именно у меня проблемы\"]})\n",
    "lem_check_df, vocabulary = to_lemmatize2(check_df)\n",
    "union_res = \" \".join(lem_check_df[\"lemmatized_text\"]).strip()\n",
    "\n",
    "print(\"Ответ к упражнению 2:\")\n",
    "print(union_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на время работы второго способа нормализации и сравним его с первым:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['после' 'оплаты' 'выполнения' ... 'знр07210187' 'знр07210190'\n",
      " 'знр07210193']\n",
      "Wall time: 7.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df, _ = to_lemmatize2(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>lowercase_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>После оплаты , выполнения ЗнЗ, не получается и...</td>\n",
       "      <td>1-1</td>\n",
       "      <td>После оплаты выполнения ЗнЗ не получается испо...</td>\n",
       "      <td>после оплаты выполнения знз не получается испо...</td>\n",
       "      <td>послепосол оплата выполнение знз не получаться...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Вопрос/проблема: Не дает исполнить ЗНС</td>\n",
       "      <td>1-1</td>\n",
       "      <td>Вопрос проблема Не дает исполнить ЗНС</td>\n",
       "      <td>вопрос проблема не дает исполнить знс</td>\n",
       "      <td>вопрос проблема не давать исполнить знс</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Вопрос/проблема: Не могу исполнить знз 2076251...</td>\n",
       "      <td>1-22</td>\n",
       "      <td>Вопрос проблема Не могу исполнить знз по догов...</td>\n",
       "      <td>вопрос проблема не могу исполнить знз по догов...</td>\n",
       "      <td>вопрос проблема не мочь исполнить знз по догов...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Вопрос/проблема: Дорбрый день! Пытаюсь провест...</td>\n",
       "      <td>1-71</td>\n",
       "      <td>Вопрос проблема Дорбрый день Пытаюсь провести ...</td>\n",
       "      <td>вопрос проблема дорбрый день пытаюсь провести ...</td>\n",
       "      <td>вопрос проблема дорбрый деньдеть пытаться пров...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>При попытке подписания документа внутри банка ...</td>\n",
       "      <td>1-71</td>\n",
       "      <td>При попытке подписания документа внутри банка ...</td>\n",
       "      <td>при попытке подписания документа внутри банка ...</td>\n",
       "      <td>приперетьпря попытка подписание документ внутр...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text class  \\\n",
       "0  После оплаты , выполнения ЗнЗ, не получается и...   1-1   \n",
       "1            Вопрос/проблема: Не дает исполнить ЗНС    1-1   \n",
       "2  Вопрос/проблема: Не могу исполнить знз 2076251...  1-22   \n",
       "3  Вопрос/проблема: Дорбрый день! Пытаюсь провест...  1-71   \n",
       "4  При попытке подписания документа внутри банка ...  1-71   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  После оплаты выполнения ЗнЗ не получается испо...   \n",
       "1              Вопрос проблема Не дает исполнить ЗНС   \n",
       "2  Вопрос проблема Не могу исполнить знз по догов...   \n",
       "3  Вопрос проблема Дорбрый день Пытаюсь провести ...   \n",
       "4  При попытке подписания документа внутри банка ...   \n",
       "\n",
       "                                      lowercase_text  \\\n",
       "0  после оплаты выполнения знз не получается испо...   \n",
       "1              вопрос проблема не дает исполнить знс   \n",
       "2  вопрос проблема не могу исполнить знз по догов...   \n",
       "3  вопрос проблема дорбрый день пытаюсь провести ...   \n",
       "4  при попытке подписания документа внутри банка ...   \n",
       "\n",
       "                                     lemmatized_text  \n",
       "0  послепосол оплата выполнение знз не получаться...  \n",
       "1            вопрос проблема не давать исполнить знс  \n",
       "2  вопрос проблема не мочь исполнить знз по догов...  \n",
       "3  вопрос проблема дорбрый деньдеть пытаться пров...  \n",
       "4  приперетьпря попытка подписание документ внутр...  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что второй метод работает намного быстрее. Так что на практике, при необходимости лемматизации слов, нужно использовать именно второй подход."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Осуществление замен"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В обращениях пользователей встречаются аббревиатуры, представляющие собой специфичные термины, которые используются только в рамках узкого бизнес-процесса, например:\n",
    "- ЗНС (заявка на снабжение)\n",
    "- ЗНВ (заявка на выбытие)\n",
    "- ЭД (электронный документооборот)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом пользователи могут использовать как абревиатуры, так и указывать полное наименование термина. Обработка этих случаев важна для классификации, поэтому необходимо приводить различные формы их написания к единой форме. С этой целью создан файл $abbr\\_df.csv$, который содержит два поля:\n",
    "- target: аббревиатура\n",
    "- sourse: выражение, которые нужно заменить на аббревиатуру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>запрос на снабжение</td>\n",
       "      <td>знс</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>запрос на выбытие</td>\n",
       "      <td>знв</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>заявка на корректировка</td>\n",
       "      <td>знк</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>запрос на расходование средство</td>\n",
       "      <td>знрс</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>заявка на платёж</td>\n",
       "      <td>знп</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              source  target\n",
       "0               запрос на снабжение     знс \n",
       "1                 запрос на выбытие     знв \n",
       "2           заявка на корректировка     знк \n",
       "3   запрос на расходование средство    знрс \n",
       "4                  заявка на платёж     знп "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abbr_df = pd.read_csv('abbr_df.csv', dtype=str)\n",
    "abbr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Упражнение 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание:** Произведите замены в тексте согласно следующему алгоритму:\n",
    "- Определите функцию, осуществляющую замену для заданного текста (используйте строковый метод $replace$)\n",
    "- Примените реализованную функцию для осуществления всех замен из таблицы $abbr\\_df$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text class  \\\n",
      "0  После оплаты , выполнения ЗнЗ, не получается и...   1-1   \n",
      "1            Вопрос/проблема: Не дает исполнить ЗНС    1-1   \n",
      "2  Вопрос/проблема: Не могу исполнить знз 2076251...  1-22   \n",
      "\n",
      "                                      tokenized_text  \\\n",
      "0  После оплаты выполнения ЗнЗ не получается испо...   \n",
      "1              Вопрос проблема Не дает исполнить ЗНС   \n",
      "2  Вопрос проблема Не могу исполнить знз по догов...   \n",
      "\n",
      "                                      lowercase_text  \\\n",
      "0  после оплаты выполнения знз не получается испо...   \n",
      "1              вопрос проблема не дает исполнить знс   \n",
      "2  вопрос проблема не могу исполнить знз по догов...   \n",
      "\n",
      "                                     lemmatized_text  \\\n",
      "0  послепосол оплата выполнение знз не получаться...   \n",
      "1            вопрос проблема не давать исполнить знс   \n",
      "2  вопрос проблема не мочь исполнить знз по догов...   \n",
      "\n",
      "                              text_with_abbreviation  \n",
      "0  послепосол оплата выполнение знз не получаться...  \n",
      "1            вопрос проблема не давать исполнить знс  \n",
      "2  вопрос проблема не мочь исполнить знз по догов...  \n"
     ]
    }
   ],
   "source": [
    "#print(df[:10])\n",
    "def abbr_func(text, source, target):\n",
    "    #Начало кода\n",
    "    return None\n",
    "    #Конец кода\n",
    "    \n",
    "def to_abbr_replace(df):\n",
    "    df[\"text_with_abbreviation\"] = df[\"lemmatized_text\"]\n",
    "    for i in range(len(abbr_df)):\n",
    "        source = abbr_df[\"source\"][i]\n",
    "        target = abbr_df[\"target\"][i]\n",
    "        #Начало кода\n",
    "        df[\"text_with_abbreviation\"] = None\n",
    "        #Конец кода\n",
    "    return df\n",
    "        \n",
    "df = to_abbr_replace(df)\n",
    "#print(df[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проверка:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_df = DataFrame({\"lemmatized_text\" : [\" запрос на снабжение \", \n",
    "                                          \" запрос на выбытие \", \n",
    "                                          \" запрос на расходование средство \"]})\n",
    "abr_check_df = to_abbr_replace(check_df)\n",
    "union_res = \" \".join(abr_check_df[\"text_with_abbreviation\"]).strip()\n",
    "\n",
    "print(\"Ответ к упражнению 3:\")\n",
    "print(union_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом предобработка обращений завершена. Мы можем удалить из таблицы $df$ столбцы, отвечающие промежуточным преобразованиям и оставить только последний вариант:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"transformed_text\"] = df[\"text_with_abbreviation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.drop([\"tokenized_text\", \"lowercase_text\", \"lemmatized_text\", \"text_with_abbreviation\"], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В конце посмотрим, насколько нам удалось сократить количество уникальных слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_n_words = len(set(\" \".join(df[\"transformed_text\"]).split()))\n",
    "print(\"количество уникальных слов до обработки обращений:\", n_words)\n",
    "print(\"количество уникальных слов после обработки обращений:\", new_n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что мы сокралили количество уникальных слов практически в два раза."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Выделение признаков (Векторизация)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перейдем к этапу выделеения признаков из текста:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/1_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделение признаков является финальной частью векторизации:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/1_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы рассмотрим два этапа отбора признаков:\n",
    "- На **первом этапе** мы отбираем большое количество признаков по частоте их встречаемости в текстах всех обращений.\n",
    "- На **втором этапе** из большого числа признаков отбираем наиболее важных признаки с помощью трех методов:\n",
    "    - Статистическое оценивание важности признака.\n",
    "    - Логистическая Регрессия (с помощью $l_1$ регуляризатора).\n",
    "    - Случайный лес (с помощью флага $oob\\_score$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После первого этапа отбора признаков мы сможем получить из исходной таблицы $df$ два объекта:\n",
    "-  $X \\in R^{n \\times m}$ - матрицу объекты-признаки ($n$ - количество объектов, $m$ - количество признаков)\n",
    "- $y \\in R^{n}$ - вектор правильных ответов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наличие пары $(X, y)$, уже позволит нам применить методы стандартного машинного обучения, которые мы обсуждали в курсе, и получить некоторое начальное решение для дальнейших методов отбора признаков. Далее каждый метод из второго этапа будет лишь уточнять матрицу объекты-признаки $X$ (а именно, удалять из нее некоторые столбцы, соответствующие неинформативным признакам). После каждого метода на паре $(\\overline{X}, y)$ с измененной матрицей объекты-признаки $\\overline{X}$, мы будем обучать те же алгоритмы машинного обучения и смотреть, как изменяется их точность. Если точность будет возрастать, то это будет с высокой вероятностью означать, что мы удалили неинформативные признаки и помогли классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основными методами проверки точности модели является использование отложенной выборки и кроссвалидации. Отложенная выборка предполагает разбиение всех имеющихся данных на две части, на одной из которых происходит обучение модели, а на другой - проверка ее точности. Кроссвалидация предполагает повтор описанной операции несколько раз - мы делим всю выборку $(X, y)$ на $N$ частей, после чего обучаемся на $N-1$ части и находим точность на оставшейся. Такую процедуру повторяют $N$ раз, после чего получают N значений точности и усредняют результат. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/1_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы сократить время обучения моделей, для оценки точности мы будем использовать отложенную выборку."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Первый этап (Отбор частотных слов +  Векторизация)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для выделения частотных признаков удобно использовать объект $CountVectorizer$. Для того, чтобы отобрать признаки по частоте, достаточно задать два важных параметра :\n",
    "- $min\\_df$: Минимальная частота, которой должно обладать слово, чтобы стать признаком.\n",
    "- $ngram\\_range$: Пара чисел, означающая какой диапазон n-грамм мы хотим использовать. В случае $ngram\\_range = (1, 1)$ в качестве кандидата на место признака будут рассматриваться только слова, а, например, при $ngram\\_range = (1, 2)$ признаками станут и слова, и биграммы, преодолевшие заданный порог частоты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для нашей задачи зададим следующие параметры: $ngram\\_range=(1, 5)$. Можно задать сколь угодно большой диапазон n-грамм не боясь сльно увеличить количество признаков, поскольку в любом случае всем n-граммам затем необходимо будет превзойти заданный порог частоты. А вот параметр $min\\_df$ необходимо выбрать более аккуратно: при слишком маленьких значениях мы получим неоправданно большое количество признаков, что осложноит дальнейшее применение моделей машииного обучения, а при слишком больших значениях мы можем упустить значимые слова, которые всречаются малое число раз."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы решить описанную неопределенность, запустим векторизацию с различными значениями $min\\_df$ и выберем минимальное, при котором матрица объекты-признаки будет приемлемого размера:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выберем параметр $min\\_df = 30$ и произведем окончательную векторизацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "MAX_N_GRAMM = 5\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=30, ngram_range=(1, MAX_N_GRAMM))\n",
    "X = vectorizer.fit_transform(df[\"transformed_text\"])\n",
    "y = df[\"class\"].values\n",
    "print(\"размерность матрицы объекты-признаки:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем все имеющиеся данные на обучающую и проверочную выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, stratify=y, test_size=0.2)\n",
    "print(\"===== полученные размерности =====\")\n",
    "print(\"X_train.shape:\", X_train.shape)\n",
    "print(\"X_dev.shape:\", X_dev.shape)\n",
    "print(\"y_train.shape:\", y_train.shape)\n",
    "print(\"y_dev.shape:\", y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим качество выбранного признакового описания. Обучим модели на обучающей выборке $(X\\_train, y\\_train)$, и проверим их точность на отложенной выборке $(X\\_dev, y\\_dev)$. Точность каждой модели на отложенной выборке можно узнать с помощью метода $accuracy\\_score$. Точность каждой модели будем сохранять в словаре $scores$. Эта информация будет необходима, когда мы будем оценивать качество последующих методов отбора признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Внимание:** обучение можеть занять достаточное время."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#methods\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#scores\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "lr = LogisticRegression()\n",
    "svc = SVC(kernel=\"linear\")\n",
    "knn = KNN(n_neighbors=1)\n",
    "nb = GaussianNB()\n",
    "\n",
    "scores = {}\n",
    "for name, clf in [(\"random forest\", rfc), \n",
    "                  (\"logistic regression\", lr),\n",
    "                  (\"SVM\", svc),\n",
    "                  (\"knn\", knn),\n",
    "                  (\"naive bayes\", nb)]:\n",
    "    if name == \"naive bayes\":\n",
    "        clf.fit(X_train.toarray(), y_train)\n",
    "        scores[name] = accuracy_score(y_dev, clf.predict(X_dev.toarray()))\n",
    "    else:\n",
    "        clf.fit(X_train, y_train)\n",
    "        scores[name] = accuracy_score(y_dev, clf.predict(X_dev))\n",
    "    print(name, scores[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы сократить время обучения моделей, далее при анализе методов машинного обучения будем использовать только две модели: $Random Forest$ и $Logistic Regression$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.2 Второй этап (Применение методов отбора признаков)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, теперь у нас есть список слов, которые стали признаками в результате частотного отбора. Создадим список таких слов в порядке их следования в матрице объекты-признаки $X$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary = list(Series(vectorizer.vocabulary_).keys())\n",
    "vocabulary[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нам необходимо произвести отбор признаков поверх частотного отбора с помощью трех методов, которые мы указывали выше. Заведем словарь, который для каждого метода будет хранить все слова-признаки с соотвествующими оценками значимости:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_imporances = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы проверить качество стандартных моделей машинного обучения после реализованного отбора признаков, будем создавать новый объект $CountVectorizer$ с заданным параметром $vocabulary$ - списком отобранных слов, которые мы хотим сделать признаками. В качестве отобранных слов мы будем выбирать первые $n\\_features$ слов в порядке их значимости согласно каждому методу, где $n\\_features$ будет принимать следующие значения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features_list = np.arange(500, 2100, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заранее напишем две функции, каждая из которых для фиксированного метода отбора признаков:\n",
    "- возвращает точность моделей для $n\\_featutres$ признаков для каждого значения $n\\_featutres$ из $n\\_features\\_list$\n",
    "- отрисовывает зависимость точности моделей от количества значимых признаков и сравненивает эту точность с точностью моделей без отбора признаков (точнее, только с частотным отбором)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_accuracy_list(feature_imporances, vocabulary=vocabulary, n_features_list=n_features_list):\n",
    "    accuracy_list = {\"random forest\" : [], \"logistic regression\" : []}\n",
    "    for n_features in n_features_list:\n",
    "        print(\"===== n_features:\", n_features, \"=====\")\n",
    "        usefull_features = feature_imporances.keys()[:n_features]\n",
    "        usefull_features_indexes = [i for i in range(len(vocabulary)) if vocabulary[i] in usefull_features]\n",
    "        #print(usefull_features_indexes)\n",
    "        decreased_X_train = X_train[::, usefull_features_indexes]\n",
    "        decreased_X_dev = X_dev[::, usefull_features_indexes]\n",
    "        for name, clf in [(\"random forest\", rfc), (\"logistic regression\", lr)]:\n",
    "            clf.fit(decreased_X_train, y_train)\n",
    "            accuracy_list[name].append(accuracy_score(y_dev, clf.predict(decreased_X_dev)))\n",
    "            \n",
    "    return accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def print_results(accurasy_list, n_features_list=n_features_list, scores=scores):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for name in accurasy_list.keys():\n",
    "        plt.plot(n_features_list, accurasy_list[name])\n",
    "    for name in accurasy_list.keys():\n",
    "        plt.plot(n_features_list, np.full(len(n_features_list), scores[name]))\n",
    "    plt.xlabel(\"количество признаков\")\n",
    "    plt.ylabel(\"точность\")\n",
    "    plt.legend(list(accurasy_list.keys()) + [name + \" без отбора признаков\" for name in list(accurasy_list.keys())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь последовательно применим каждый из методов отбора признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Статистическое оценивание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напомним, что в статистическом оценивании мы проеряем следующие гипотезы для каждого признака:\n",
    "- гипотеза $H_0$ : признак является незначимым (признак и целевая переменная являются независимыми случайными величинами)\n",
    "- гипотеза $H_1$ : признак значим"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После проверки гипотезы может сформироваться 4 возможные ситуации:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/1_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По величине $p-value$ мы можем сделать вывод о значимости признака:\n",
    " - если $p-value$ принимает относительно большие значения, то это означает следующее: большая вероятность того, что гипотеза $H_0$ неверно отвергнута. То есть, большая вероятность того, что $H_0$ верна и признак с большой вероятность является незначимым.\n",
    " - если $p-value$ принимает относительно малые значения, то это означает, что вероятность того, что гипотеза $H_0$ неверно отвергнута мала. То есть $H_0$ отвергнута по существу и признак с высокой вероятностью значим."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем статистический метод отбора признаков применительно к нашим данным:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем величины $p-value$ для каждого признака с помощью метода $chi2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "p_values = chi2(X_train, y_train)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем отсортируем все признаки по величинам $p-value$ и посмотрим на несколько первых и несколько последних признаков в этом списке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_imporances[\"p-value\"] = Series(p_values, index=vocabulary).sort_values()\n",
    "feature_imporances[\"p-value\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_imporances[\"p-value\"][-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кажется, что отбор признаков прошел успешно. Проверим, точность моделей на сокращенном признаковом описании:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "accurasy_list = get_accuracy_list(feature_imporances[\"p-value\"])\n",
    "print_results(accurasy_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Оценивание с использованием Случайного леса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним схему работу случайного леса в случае, когда мы устанавливаем флаг $oob\\_score$ значением $True$:\n",
    "- Все множество объектов $X$ разбивается на два множества : $X'$ и $X''$\n",
    "- Множество $X'$ используется при обучении (по случайному подмножеству признаков)\n",
    "- Множество $X''$ используется при проверке качества (в частности, качества выбранных признаков)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/1_9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Произведем описанную процедуру:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(oob_score=True)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отсортируем признаки и посмотрим на самые значимые и самые незначимые из них:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_imporances[\"random forest with oob\"] = Series(clf.feature_importances_, \n",
    "                                                      index=vocabulary).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_imporances[\"random forest with oob\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_imporances[\"random forest with oob\"][-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим точность моделей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rfc_accurasy_list = get_accuracy_list(feature_imporances[\"random forest with oob\"])\n",
    "print_results(rfc_accurasy_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Оценивание с использованием Логистической регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценивание с использование логистической регрессии подразумевает использованим  $l_1$ регуляризатора. Суть регуляризаторов в следующем. Решающее правило для логичтисеко регрессии имеет следующий вид:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a(x) = sign(<w, x> - b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для применения метода, необходимо настроить параметры $(w, b)$, что и присходит при обучении путем минимизации функции потерь:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(w, b) \\rightarrow min\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, может случиться так, что функция $L(w, b)$ принимает малые значения на обучающей выборке, но на отложенных данных (или на кроссвалидации), точность модели падает. Это говорит о переобучении. Чтобы избежать такого эффекта, к минимизируемой функции $L(w)$ добавляет слагаемое, которое зависит только от вектора весов $w$. Основными типами регуляризации является $l_1$ и $l_2$ регуляризация:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "l_1 : L(w, b) + C \\cdot \\sum{|w_i|} \\rightarrow min\n",
    "$$\n",
    "$$\n",
    "l_2 : L(w, b) + C \\cdot \\sum{w_i^2} \\rightarrow min\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отбору признаков способствует именно $l_1$ регуляризация. Отбор признаков происходит за счет того, что некоторые компоненты вектора весов $w$ становятся равны нулю, в результате чего соотвествующие признаки не принимают участия в классификации. На количество оставшихся признаков вляет константа $C$ : чем она больше, тем больше значение придается регуляризации и тем меньше признаков остается. Мы будем использовать значение $C = 0.1$, но Вы можете изменить его и посмотреть, как это будет влиять на результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C = 0.1\n",
    "clf = LogisticRegression(penalty='l1', C=C)\n",
    "clf.fit(X_train, y_train)\n",
    "feature_imporances[\"logistic regression\"] = Series(np.abs(clf.coef_).sum(0), \n",
    "                                                                    index=vocabulary).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на \"лучшие\" и \"худшие\" слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_imporances[\"logistic regression\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_imporances[\"logistic regression\"][-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим точность моделей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "log_reg_accurasy_list = get_accuracy_list(feature_imporances[\"logistic regression\"])\n",
    "print_results(log_reg_accurasy_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В дальнейшем, при оптимизации и смешивании алгоритмов мы будем использовать 1200 признаков, отобранные логистической регрессией. Вы можете использовать другое количество признаков при другом методе отбора, если в ваших экспериментах другие методы показывают лучший результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usefull_features = feature_imporances[\"logistic regression\"][:1200]\n",
    "usefull_features_indexes = [i for i in range(len(vocabulary)) if vocabulary[i] in usefull_features]\n",
    "#print(usefull_features_indexes)\n",
    "dec_X_train = X_train[::, usefull_features_indexes]\n",
    "dec_X_dev = X_dev[::, usefull_features_indexes]\n",
    "print(\"===== полученные размерности =====\")\n",
    "print(\"dec_X_train.shape:\", dec_X_train.shape)\n",
    "print(\"dec_X_dev.shape:\", dec_X_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оптимизация алгоритмов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сокращение признакового опиcание не является единственным методом увеличения точности моделей. Вспомним слайд из лекций:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/1_10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так что после отбора признаков попробуем оптимизировать каждую из моделей машинного обучения за счет оптимального выбора гиперпараметров. Вспомним основные гиперпараметры каждого из методов:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/1_11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует два метода отбора гиперпараметров:\n",
    "- Перебор по сетке (перебираются всевозможные наборы указанных гиперпараметров)\n",
    "- Рандомизированный перебор (перебирается случайная часть наборов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашем практическом задании мы будем использовать перебор по сетке. Рандомизированный перебор реализуется аналогичным способом. Вспомним последовательность действий при осуществлении перебора по сетке:\n",
    "- Задаем значения параметров, которые хотим перебрать\n",
    "- Создаем объект $GridSearchCV$\n",
    "- Перебираем все наборы гиперпараметров\n",
    "- Получаем лучший алгоритм"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшие алгоритмы будем записывать в словарь $best\\_clf$, а соотвествующие точности на отложенной выборке $best\\_accuarcy$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_clf = {}\n",
    "best_accuracy = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По описанному плану произведем оптимизацию логистической регрессии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\"C\" : [0.01, 0.1, 1], \"penalty\" : [\"l1\", \"l2\"]}\n",
    "lr_clf = LogisticRegression()\n",
    "lr_gs = GridSearchCV(clf, param_grid)\n",
    "lr_gs.fit(dec_X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на оптимальные гиперпараметры и точность лучшего алгоритма на кроссвалидации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_gs.best_params_, lr_gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним лучшую модель и ее точность на отложенной выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_clf[\"logistic regression\"] = lr_gs.best_estimator_\n",
    "best_accuracy[\"logistic regression\"] = accuracy_score(y_dev, best_clf[\"logistic regression\"].predict(dec_X_dev))\n",
    "print(\"optimized logistic regression accuracy:\", best_accuracy[\"logistic regression\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Упражнение 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание:** Реализуйте аналогичный перебор гиперпараметров для модели $Random Forest$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Замечание:** У модели $Random Forest$ не нужно указывать большие значения для гиперпараметра $n\\_estimatiros$. Можно оптимизировать все остальные гиперпараметры при небольшом количестве деревьев (например, при $n\\_estimatiros = 100$), зафиксировать их и затем увеличить количество деревьев."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним лучшую модель и ее точность на отложенной выборке:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Random Forest$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Начало кода\n",
    "\n",
    "## Конец кода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрите на оптимальные гиперпараметры и точность лучшего алгоритма на кроссвалидации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Начало кода\n",
    "\n",
    "## Конец кода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Увеличьте количество деревьев в ансамбле:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Начало кода\n",
    "\n",
    "## Конец кода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраните обученную модель и ее точность на отложенной выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Начало кода\n",
    "\n",
    "## Конец кода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проверка:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert best_accuracy[\"random forest\"] > 0.65\n",
    "print(\"Реузльтат неоптимизированной модели превзойден!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Смешивание алгоритмов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того, как мы выбрали признаковое описание и оптимизировали каждый из алгоритмов, мы можем попытать улучшить качество моделей за счет объедиения нескольких алгоритмов. Вспомним основные методы объединения:\n",
    "- Объединение по предсказаниям (из ответов нескольких алгоритмов выбираем то, которое встречалось чаще других)\n",
    "- Объединение по вероятностям"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первый вариант объединения имеет несколько недостатков: \n",
    "- Неоднозначность классификации (если модели отдают равное количество голосов за несколько классов)\n",
    "- Трудности с определением вероятности предсказания\n",
    "\n",
    "Так что в нашем практическом задании мы будем реализовывать втрой метод. Пусть мы имеем два обученных алгоритма: $p_1(x)$ и $p_2(x)$. Тогда для их объекдинения по вероятностям необходимо следующее:\n",
    "- Создаем модель, объединяющую две модели машинного обучения\n",
    "- Задаем параметр смешивания $\\alpha$\n",
    "- Выдаем предсказание по правилу $\\alpha \\cdot p_1(x) + (1 - \\alpha) \\cdot p_2(x) $\n",
    "- Настраиваем гиперпараметр $\\alpha$ с помощью перебора по сетке\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем класс для объединения двух моделей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class Mixer(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, clf1, clf2, alpha=0.5, threshold=0.5, classes=np.array(sorted(list(set(y_train))))):\n",
    "        self.clf1 = clf1\n",
    "        self.clf2 = clf2\n",
    "        self.alpha = alpha\n",
    "        self.classes = classes\n",
    "        \n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.clf1.fit(X, y)\n",
    "        self.clf2.fit(X, y)\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        return self.clf1.predict_proba(X) * self.alpha + self.clf2.predict_proba(X) * (1 - self.alpha)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        prob_prediction = self.predict_proba(X)\n",
    "        \n",
    "        return self.classes[prob_prediction.argmax(1)]\n",
    "    \n",
    "    def get_params(self, deep = False):\n",
    "        \n",
    "        return {'clf1' : self.clf1, 'clf2' : self.clf2, 'alpha' : self.alpha}\n",
    "    \n",
    "    def set_params(self, alpha):\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединим оптимизированный случайный лес с оптимизированной логистической регрессией:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mix = Mixer(best_clf[\"random forest\"], best_clf[\"logistic regression\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметр $alpha$ можно настроить с помощью перебора по сетке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "## Начало кода\n",
    "param_grid = {\"alpha\" : [np.arange(0, 1.1, 0.2)]}\n",
    "mix_gs = GridSearchCV(mix, param_grid)\n",
    "mix_gs.fit(dec_X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но в целях экономии времени мы не будем этого делать. Подставим параметр $alpha = 0.3$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "mix = Mixer(best_clf[\"random forest\"], best_clf[\"logistic regression\"], alpha=0.3)\n",
    "mix.fit(dec_X_train, y_train)\n",
    "accuracy_score(mix.predict(dec_X_dev), y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на результат на отложенной выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(mix.predict(dec_X_dev), y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Упражнение 5\n",
    "**Задание:** Смешайте только что полученную смесь логистичсекой регрессии и случайного леса с любым другим алгоритмом. Параметры добавленного алгоритма и новую величину $alpha$ можно настроить либо перебором по сетке или рандомизированным перебором (что займет достаточное количество времени), либо подобрать вручную. Удается ли улучшить результат?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Начало кода\n",
    "\n",
    "##Конец кода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нейронная сеть прямого распространения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом пункте мы попробуем применить к решению задачи нейронные сети прямого распространения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/1_12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изначально необходимо определить архитектуру сети:\n",
    "- Количество слоев в нейронной сети\n",
    "- Количество нейронов на каждом слое\n",
    "- Функции активации для каждого слоя"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того, как определена архитектура, мы можем приступить к обучению сети. Обучение сети состоит из прямого и обратного распространения ошибки:\n",
    "- Во время **прямого распространения** для каждого объекта мы должны посчитать значения всех нейронов слева направо, найти предсказание нейронной сети. Затем, на основе предсказаний посчитать функцию потерь для каждого объекта, а затем и суммарную функцию потерь\n",
    "- После того, как мы нашли функцию потерь, нам необходимо найти ее производные, чтобы применить метод градиентного спуска. Производные функции потерь вычисляются ноборот, справа налево, что происходит во время **обратного распространения**. С помощью вычисленных производным мы можем совершить итерацию градиентного спуска и обновить внутренние параметры сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/1_13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем небольшую нейронную сеть прямого распространения с помощью фреймворка $keras$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим архитектуру сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.layers as L\n",
    "from keras.models import Sequential\n",
    "\n",
    "N_FEATURES = X_train.shape[1]\n",
    "N_CLASSES = len(set(y_train))\n",
    "ffnn = Sequential()\n",
    "ffnn.add(L.Dense(N_CLASSES, input_shape = (N_FEATURES,)))\n",
    "ffnn.add(L.Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описание полученной архитектуры можно получить с помощью метода $summary$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ffnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нам необходим скомпилировать нашу модель. Компиляция включает в себя:\n",
    "- Вид функции потерь\n",
    "- Вид оптимизатора градиентного спуска\n",
    "- Метрику (необходимо, чтобы проверять точность нейронной сети на отложенной выборке после каждой эпохи)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ffnn.compile(loss='categorical_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем начать обучать нашу сеть с помощью метода $fit$. Параметры метода:\n",
    "- Обучающая выборка $(X\\_train, y\\_train)$\n",
    "- Размер батча $batch\\_size$\n",
    "- Проверочная выборка $(X\\_dev, y\\_dev)$ (для оценивания точности после каждой эпохи)\n",
    "- Количество эпох $epochs$\n",
    "- Параметр $verbose$ - нужно ли выводить информацию об обучении ($verbose = 1$) или нет ($verbose = 0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import get_dummies\n",
    "\n",
    "ffnn.fit(X_train.toarray(), get_dummies(y_train).values,\n",
    "         batch_size=256,\n",
    "         validation_data = (X_dev.toarray(), get_dummies(y_dev).values),\n",
    "         epochs=20,\n",
    "         verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Упражнение 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание:** Реализуйте собственную архитектуру нейронной сети прямого распространения и попытайтесь уровень в 74% правильно классифицированных обращений. Рекомендации к реализации:\n",
    "- Реализуйте более сложные архитектуры (увеличьте количество слоев, количество нейронов на каждом слое). Учитывайте, что обычно увеличивают количество слоев сети, а затем количество нейронов на псоелднем слое. Удается ли повысить точность модели за счет усложнения ее архитектуры? До какого момента растет точность на отложенных данных?\n",
    "- Попробуйте различные функции активации скрытых слоев ($relu$, $tanh$)\n",
    "- Опредлите, ваша модель недообучается (точность на тренировочносй выборке мала) или переобучается(точность на тренировочном множестве мала, но точность на отложенной выборке велика)\n",
    "- В случае недообучения используйте следующие методики:\n",
    "    - Инициализациz полносвязных слоев (параметры $kernel\\_initializer$ и $bias\\_initializer$)\n",
    "    - Оптимизаторы градиентного спуска ($adam$ во многоих задачах являестя оптимальным решением, но Вы можете использовать и другие оптимизаторы: $adadelta$, $RMSprop$ и другие)\n",
    "    - Нормализация весов (между полносвязными слоями Вы можете добавить слой $L.BatchNormalization()$)\n",
    "- В случае переобучения:\n",
    "    - Введение регуляризации ($kernel\\_regularizer$ и $bias\\_regularizer$)\n",
    "    - Введение $dropout$ слоя ($L.Dropout()$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Начало кода\n",
    "\n",
    "#Конец кода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ffnn.compile(loss='categorical_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = ffnn.fit(X_train.toarray(), get_dummies(y_train).values,\n",
    "         batch_size=256,\n",
    "         validation_data = (X_dev.toarray(), get_dummies(y_dev).values),\n",
    "         epochs=20,\n",
    "         verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проверка:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert history.history[\"val_acc\"][-1] > 0.74\n",
    "print(\"Проверка пройдена!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN with Word2Vec embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом пункте мы применим двунаправленную нейронную сеть долгой краткосрочной памяти, на вход которой будут подаватьcz Word2Vec представления слов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прежде чем присутпать к реализации рекурентной нейронной сети, нам необходимо векторизовать корпус обращений с помощью Word2Vec представлений. Напомним, что суть Word2Vec представлений в том, чтобы сопоставить каждому слову обращения вектор, который обладает следующими свойствами:\n",
    "- Имеет небольшую размерность по сравнению с размерностью словаря\n",
    "- Вектора для близких по смыслу слов близки друг к другу, а вектора для далеких по смыслу слов - далеки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/1_14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы получить векторные представления, обладающие таким свойством, во всех обращениях мы выделяем центральные слова и соотвествующие контексты слов:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/1_15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сопоставим каждому слову два вектора:\n",
    "- вектор $u \\in R^d$ в случае, когда слово является центральным\n",
    "- вектор $v \\in R^d$ в случае, когда слово является контекстом некоторого центрального слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем описать меру близости центрального слова словам контекста как меру близости соотвествующих векторов. Осталось найти пару векторов ($u$, $v$) для каждого слова. Существуют две основные модели для обучения:\n",
    "- $CBOW$ (пытаемся предсказать центральное слово по кнтексту)\n",
    "- $Skip-gramm$ (пытаемся предсказать слова контекста по центральному слову)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После обучения мы получим вектора ($u$, $v$) со следующим свойством: вектор $u$ некоторого слова $w$ будет близок к таким векторам $v_i$ слов $w_i$, с которыми он часто встречался в одном контексте (слова находились недалеко друг от друга). Поскольку близким по смыслу словам соответвуют близкие контексты, то для двух близких по смыслу слов $w_1$ и $w_2$ соотвествующие вектора $u_1$ и $u_2$ будут близки примерно к одним и тем же векторам $v_i$, что обязывает вектора $u_1$ и $u_2$ быть близкими друг другу. Именно по векторам $u$ после обучения и опредляется близость слов по смыслу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обучим Word2Vec на нашем корпусе обращений. Стоит отметить, что в NLP задачах часто используются предобученные векторные представления - такие представления обучаются на большом корпусе текстов и являются более точными. Однако в нашем курсе мы будем использовать простой вариант, когда вектора обучаются на целевом корпусе обращений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = [doc.split() for doc in df['transformed_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение Word2Vec на полученном корпусе:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "w2v = Word2Vec(sentences=docs, size=EMBEDDING_DIM, window=4, sg=0, iter=35, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посмотрим на качество полученных векторных представлений: для слов посмотрим ближайшие к ним слова в смысле близости между Word2Vec векторами. Это можно сделать с помощью метода $most\\_similar$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v.most_similar('ошибка')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v.most_similar('знс')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reccurent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того, как получены векторные представления, мы можем использовать рекурентные нейронные сети для решения нашей задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основным минусом нейронных сетей прямого распространения является тот факт, что такие сети не учитывают порядок слов в обращении. Рекуррентные нейронные сети ведут такой порядок. Вспомним схему их работы:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/1_16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задаче классификации текстов интересная именно схема $Many-to-one$, когда несколким входам нейронной сети соотвествует обин выход - класс обращения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть наше обращение состоит из слов $[w_1, w_2, ..., w_{T_X}]$ и  $[x_1, x_2, ..., x_{T_X}]$ - соотвествующие векторные представления. Суть рекурентной нейронной сети в том, чтобы пройтись по векторным представлениям слева направо, в каждый момент времени формируя активацию нейронную сети $a^{<t>}$. Изначально мы инициализируем активацию случайным образом, а затем на каждом шаге пересчитываем, учитывая векторное представление текущего слова и прошлую активацию согласно следующией формуле:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a^{<t>} = g(W_{a} \\cdot a^{<t - 1>} + W_{x} \\cdot x^{<t - 1>} + b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "где матрицы $W_{a}, W_{x}$ и вектор $b$ - внутренние параметры нейронной сети, $g$ - некоторая функция активации. Активация последнего слоя может быть соединена полносвязным слоем на количество классов в нашей задаче. Так выглядит стандартная сеть RNN применительн к задаче текстовой классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако рекуррентные нейронные сети имеют свои недостатки. Главный из них - сети такого типа плохо определяют долгосрочные зависимости. Эту проблему решает сеть LSTM за счет введения дополнительной ячейки $c$, отвечающей за память нейронной сети, а также в ведеении трех вентилей: вентиля забывания $\\Gamma_f$, вентиля обновления $\\Gamma_u$ и выходного вентиля $\\Gamma_o$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изначально мы считаем некоторого кандидата на место вектора пямяти ячейки $c$ - вектор $\\overline c$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\overline c^{<t>} = g(W_{ca} \\cdot a^{<t - 1>} + W_{cx} \\cdot x^{<t - 1>} + b_c)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем мы вычисляем вентиль забывания $\\Gamma_f$ и вентиль обновления $\\Gamma_u$, которые говорят на о том, стоит ли обновлять текущее значение ячейки $c$ значением $\\overline c$ или нет:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Gamma_u = g(W_{ua} \\cdot a^{<t - 1>} + W_{ux} \\cdot x^{<t - 1>} + b_u)\n",
    "$$\n",
    "$$\n",
    "\\Gamma_f = g(W_{fa} \\cdot a^{<t - 1>} + W_{fx} \\cdot x^{<t - 1>} + b_f)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С учетом вычисленных вентилей мы обновляем вектор памяти ячейки $c$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "c^{<t>} = \\Gamma_u \\cdot \\overline c^{<t>} + \\Gamma_f \\cdot c^{<t-1>}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем мы вычисляем последний из вентилей - вентиль обновления $\\Gamma_o$, который отвечает за то, в какой мере необходимо учитывать вектор памяти $c^{<t>}$ при вычислении активации $a^{<t>}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a^{<t>} = \\Gamma_o \\cdot g(c^{<t>})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так работает сеть LSTM. Последним улучшением является двунаправленная сеть BiLSTM, которая осуществляет прямой, а затем и обратный проход по тексту. Использование BiLSTM может сработать лучше LSTM, когда при анализе слова сущсетвует зависимость от будущего контекста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь применим рекурентную нейронную сеть в нашей задаче. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае рекуррентной нейронной сети нам недостаточно матрицы объекты-признаки $X$. с которой оперировали и стандартные методы машинного обучения, и нейронные сети прямого распространения, поскольку нам необходимо каждое слово в каждом обращении с учетом порядка слов. Так что мы возвращаемся к таблице с предобратонным текстом $df$.\n",
    "Вновь разобьем все обращения (а не из признаковые описания) на обучающую и проверочную выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(df, df[\"class\"], stratify=df[\"class\"], test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несмотяр на то, что теперь мы работаем с исходными обращениями, на вход рекуррентной нейронной сети мы по-прежнему должны подавать информацию в матричном виде. Чтобы преобразовать информацию о всех словах во всех обращениях в матричную форму, необходимо поступить следующим образом:\n",
    "- Каждое слово заменить его номером в словаре.\n",
    "- Жестко зафиксировать количество слов в обращении. Обращения, содержащие большее количество слов, обрезаются, а обращения, содержащие меньшее число слов, дополняются справа словом, имеющим нулевой номер в словаре."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Произведем описанные процедуры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train[\"transformed_text\"])\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train[\"transformed_text\"])\n",
    "data_train = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "sequences_dev = tokenizer.texts_to_sequences(X_dev[\"transformed_text\"])\n",
    "data_dev = pad_sequences(sequences_dev, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_train.shape, data_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку на вход рекурентной нейронной сети приходят номера слов в словаре, то первым делом необходима преобразовать эти номера в соотвествующие векторные представления. Подготовим матрицу, которой затем инициализируем слой, отвечающий за такое преобразование:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.random((len(tokenizer.word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in w2v.wv.vocab.keys():\n",
    "        embedding_vector = w2v.wv.word_vec(word)\n",
    "    else:\n",
    "        embedding_vector = np.zeros(EMBEDDING_DIM)\n",
    "    embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь построим архитекутур рекурентной нейронной сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "import keras.layers as L\n",
    "\n",
    "layer_input = L.Input((MAX_SEQUENCE_LENGTH,))\n",
    "layer_emb = L.Embedding(len(tokenizer.word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)(layer_input)\n",
    "layer_lstm = L.RNN(L.SimpleRNNCell(128))(layer_emb)\n",
    "layer_output = L.Dense(N_CLASSES, activation='softmax')(layer_lstm)\n",
    "\n",
    "lstm = Model(layer_input, layer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скомпилируем модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm.compile(\n",
    "    loss = 'categorical_crossentropy',\n",
    "    optimizer = 'adam',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И запустим обучение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lstm.fit(\n",
    "    data_train, pd.get_dummies(y_train),\n",
    "    validation_data = (data_dev, pd.get_dummies(y_dev)),\n",
    "    epochs = 50,\n",
    "    batch_size = 128,\n",
    "    shuffle = True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Упражнение 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание:** Реализуйте собственную архитектуру рекуррентной нейронной сети и попытайтесь превзойти прежний уровень в 74% правильно классифицированных обращений. Рекомендации к реализации:\n",
    "- Попробуйте варьировать размерность вектора $a^{<t>}$ (параметр $units$)\n",
    "- Вместо $RNN$ ячейки используйте ячейки $LSTM$ или $BiLSTM$ ($L.LSTM$, $L.Bidirectional(L.LSTM)$)\n",
    "- Попробуйте добавить несколько полносвязных слоев к последней активации. При добавлении можно использовать все рекомендации из управжнения 6\n",
    "- Попытайтесь учитывать не только последнюю активацию, а вссе активации нейронной сети (параметр $return\\_sequnces$ у $LSTM$ ячейки). Получается ли увеличить точность за счет такого учета?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Начало кода\n",
    "\n",
    "#Конец кода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm.compile(\n",
    "    loss = 'categorical_crossentropy',\n",
    "    optimizer = 'adam',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = lstm.fit(\n",
    "    data_train, pd.get_dummies(y_train),\n",
    "    validation_data = (data_dev, pd.get_dummies(y_dev)),\n",
    "    epochs = 50,\n",
    "    batch_size = 128,\n",
    "    shuffle = True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проверка:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert history.history[\"val_acc\"][-1] > 0.74\n",
    "print(\"Проверка пройдена!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
