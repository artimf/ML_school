{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Описание задания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Управление портфелем ценных бумаг необходимо для успешного сохранения и преумножения свободного капитала, которым обладает финансовая компания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Процесс активного управления портфелем ценных бумаг можно условно разделить на стратегический и тактический:\n",
    "* Стратегическое управление нацелено на выбор класса активов для достижения желаемого соотношения риск-доходность. Например, акции регулируемых компаний электроэнергетического сектора могут быть включены в портфель для снижения рыночного риска.\n",
    "* Цель тактического управления состоит в выборе таких компаний электроэнергетического сектора, которые бы максимизировали финансовый результат портфеля."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Процесс тактического управления на практике осложняется существованиям ряда несовершенств финансовых рынков, таких как наличие брокерской комиссии, которую необходимо платить при каждой ребалансировке портфеля. Поэтому, тактическое управление портфелем ценных бумаг представляет собой задачу оптимального управления."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном задании предлагается реализовать решение упрощенной задачи тактического управления портфелем ценных бумаг с помощью глубокого Q-обучения обучения. В качестве доступных финансовых активов рассматриваются следующие:\n",
    "* Обыкновенные акции компании Public Service Enterprise Group (\"PEG\")\n",
    "* Обыкновенные акции компании Consolidated Edison (\"ED\")\n",
    "* Обыкновенные акции компании Eversource Energy (\"ES\")\n",
    "* Безрисковый актив с фиксированной доходностью 1.0% годовых (\"rf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучения AI агента, осуществляющего оптимальное тактическое управление портфелем, была собрана выборка дневных наблюдений за период с 1 июля 2008 г. по 30 декабря 2016 г. Данные включают в себя цены акций, а также набор предиктивных факторов, которые могут быть полезны для предсказания доходности акций. Брокерская комиссия составляет 0.05% от размера сделки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель тактического управления портфелем – максимизировать коэффициент Шарпа (т.е. риск-взвешенную доходность портфеля). Формула расчета коэффициента Шарпа имеет следующий вид:\n",
    "![Коэффициент Шарпа](Pic1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense, LSTM, concatenate\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_session(tf.Session())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Параметры обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Безрисковая процентная ставка на 1 год\n",
    "risk_free = 0.01\n",
    "\n",
    "# Брокерская комиссия в размере 0.05% от размера сделки\n",
    "transaction_cost = 0.0005\n",
    "\n",
    "# Количество рабочих дней в году\n",
    "trading_days = 250                          \n",
    "\n",
    "# Параметр адаптации дифференциального коэффициента Шарпа (задает ширину скользящего окна)\n",
    "theta = 0.02\n",
    "\n",
    "# Параметр скорости обучения\n",
    "learn_rate = 0.1\n",
    "\n",
    "# Коэффициент дисконтирования будущей доходности\n",
    "gamma = np.exp(-risk_free/trading_days)\n",
    "\n",
    "# Количество рекуррентных ячеек в Q-сети\n",
    "rnn_layers = 5\n",
    "\n",
    "# Ширина скрытого слоя Q-сети\n",
    "n_hidden = 15\n",
    "\n",
    "# Размер батча для обучения Q-сети\n",
    "batch_size = 64\n",
    "\n",
    "# Процедура идентификации: ?-greedy с убыванием до ?=0.001 в течение 1000 эпизодов\n",
    "Epsilon_decay_periods = 1000\n",
    "Epsilon_start = 1.000\n",
    "Epsilon_end = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка предиктивных факторов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве предиктивных факторов используются следующий показатели:\n",
    "* Цены на нефть, газ, уголь и уран\n",
    "* Кривая ставок гос. облигаций США\n",
    "* Финансовые индексы S&P 500, VIX и 5-летняя ожидаемая инфляция\n",
    "* Средняя температура, скорость ветра и уровень осадков в регионе\n",
    "* Дневной прогноз спроса и рыночные цены на электроэнергию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Features = pd.read_csv('Features.csv', parse_dates = ['date_']).set_index('date_')\n",
    "\n",
    "# Лог-дифференцирование временных рядов для повышения их стационарности\n",
    "Features['SP500_Adj_Close'] = np.log(Features['SP500_Adj_Close']) - np.log(Features['SP500_Adj_Close'].shift(1))\n",
    "Features['WTI'] = np.log(Features['WTI']) - np.log(Features['WTI'].shift(1))\n",
    "Features['Natural_Gas'] = np.log(Features['Natural_Gas']) - np.log(Features['Natural_Gas'].shift(1))\n",
    "Features['Coal_Price'] = np.log(Features['Coal_Price']) - np.log(Features['Coal_Price'].shift(1))\n",
    "Features['Uranium_Price'] = np.log(Features['Uranium_Price']) - np.log(Features['Uranium_Price'].shift(1))\n",
    "Features['VIX'] = np.log(Features['VIX']) - np.log(Features['VIX'].shift(1))\n",
    "Features['Electricity_NE_Price'] = (np.log(Features['Electricity_NE_Price']) - \n",
    "                                    np.log(Features['Electricity_NE_Price'].shift(1)))\n",
    "Features['Electricity_PJM_Price'] = (np.log(Features['Electricity_PJM_Price']) - \n",
    "                                     np.log(Features['Electricity_PJM_Price'].shift(1)))\n",
    "Features['Temperature'] = np.log(Features['Temperature']) - np.log(Features['Temperature'].shift(1))\n",
    "Features['Wind_Speed'] = np.log(1+Features['Wind_Speed']) - np.log(1+Features['Wind_Speed'].shift(1))\n",
    "Features['Precipitation'] = np.log(1+Features['Precipitation']) - np.log(1+Features['Precipitation'].shift(1))\n",
    "Features['Load_Forecast'] = np.log(Features['Load_Forecast']) - np.log(Features['Load_Forecast'].shift(1))\n",
    "\n",
    "Features.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка цен финансовых активов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PEG = pd.read_csv('Prices_PEG.csv', parse_dates = ['Date'])\n",
    "ED = pd.read_csv('Prices_ED.csv', parse_dates = ['Date'])\n",
    "ES = pd.read_csv('Prices_ES.csv', parse_dates = ['Date'])\n",
    "\n",
    "# Расчет дневных доходностей акций\n",
    "PEG['PEG'] = np.log(PEG['Adj Close']) - np.log(PEG['Adj Close'].shift(1))\n",
    "ED['ED'] = np.log(ED['Adj Close']) - np.log(ED['Adj Close'].shift(1))\n",
    "ES['ES'] = np.log(ES['Adj Close']) - np.log(ES['Adj Close'].shift(1))\n",
    "\n",
    "# Расчет дневной доходности безрискового актива \n",
    "rf = pd.Series(len(Features.index)*[np.exp(risk_free/trading_days)-1], index = Features.index, name = 'rf')\n",
    "\n",
    "# Сопоставление доходностей с предиктивными факторами\n",
    "PEG = PEG.set_index('Date')['PEG'].loc[Features.index]\n",
    "ED = ED.set_index('Date')['ED'].loc[Features.index]\n",
    "ES = ES.set_index('Date')['ES'].loc[Features.index]\n",
    "\n",
    "# Объединение доходностей в один DataFrame\n",
    "Returns = pd.DataFrame([PEG, ED, ES, rf]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выделение обучающей и тестовой выборки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве тестовой выборки будут использоваться данные за 2016 г."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Дата, с которой начинается тестовый период\n",
    "test_split_date = '2016-01-01'\n",
    "\n",
    "# Количество наблюдений в обучающей выборке\n",
    "train_samples_num = len(Returns[Returns.index < test_split_date].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дискретизация допустимых структур портфеля "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для упрощения задачи предполагается, что структура портфеля ценных бумаг может принимать небольшое количество дискретных значений. С этой целью рассматриваются всевозможные сочетания из 4-х активов, в которых портфель разбивается пропорционально. Количество допустимых структур портфеля при таком подходе может быть посчитано по формуле:\n",
    "![Коэффициент Шарпа](Pic2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asset_list = ['PEG', 'ED', 'ES', 'rf']\n",
    "\n",
    "# Генерация всевозможных сочетаний из 4-х активов\n",
    "Asset_combinations = []\n",
    "for i in range(1, len(Asset_list)+1):\n",
    "    Asset_combinations.extend(it.combinations(Asset_list, i))\n",
    "\n",
    "# Пропорциональное разбиение портфеля для каждого сочетания\n",
    "Actions_list = []\n",
    "for Asset_combination in Asset_combinations:\n",
    "    Action = dict((a, 0) for a in Asset_list)\n",
    "    for a in Asset_combination:\n",
    "        Action[a] = 1 / len(Asset_combination)\n",
    "    Actions_list.append(Action)\n",
    "Actions = pd.DataFrame(Actions_list)\n",
    "\n",
    "# Вывод множества допустимых структур портфеля \n",
    "print (\"Допустимые веса активов в портфеле:\")\n",
    "Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция создания рекуррентной Q-сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве Q-сети используется гибридная рекуррентная нейронная сеть:\n",
    "* Предиктивные факторы подаются на рекуррентный слой LSTM, поскольку они обладают свойством частичной наблюдаемости (т.е. их прошлые значения также могут являться сигналами роста или падения стоимости финансовых активов).\n",
    "* Текущая конфигурации портфеля подается на Dense слой, потому что она известна достоверно.\n",
    "Архитектура Q-сети изображена на следующем графике:\n",
    "![Архитектура Q-сети](Pic3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_Q_network(features_num, assets_num, rnn_layers, n_hidden, actions_num):\n",
    "    \n",
    "    # Входной слой сети\n",
    "    Factors = Input(shape=(1, features_num), name='Factors')\n",
    "    Portfolio_Structure = Input(shape=(assets_num, ), name='Portfolio_Structure')\n",
    "\n",
    "    # Первый слой сети (конкатенация LSTM и Dense слоев)\n",
    "    Factors_first_layer = LSTM(rnn_layers)(Factors)\n",
    "    Portfolio_Structure_first_layer = Dense(assets_num)(Portfolio_Structure)\n",
    "    first_layer = concatenate([Factors_first_layer, Portfolio_Structure_first_layer])\n",
    "\n",
    "    #####################################################################\n",
    "    #################### Начало оцениваемого задания ####################\n",
    "    #####################################################################\n",
    "    \n",
    "    # Создайте второй слой сети. Тип слоя - Dense. Функция активации - relu. Ширина - n_hidden.\n",
    "    \n",
    "    second_layer = None\n",
    "    \n",
    "    #####################################################################\n",
    "    #################### Конец оцениваемого задания #####################\n",
    "    #####################################################################\n",
    "    \n",
    "    # Выходной слой сети\n",
    "    output_layer = Dense(actions_num, activation='relu', name='output_layer')(second_layer)\n",
    "\n",
    "    # Компиляция модели\n",
    "    model = Model(inputs=[Factors, Portfolio_Structure], outputs=output_layer)\n",
    "    model.compile(loss='mse', optimizer=Adam())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция вычисления дифференциального коэффициента Шарпа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В процессе обучения Q-сети в качестве функции наград удобно использовать дифференциальный коэффициент Шарпа. По сути это обычный коэффициент Шарпа, но посчитанный на некотором скользящем окне прошлых наблюдений. Ширина окна регулируется параметром адаптации theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def differential_sharpe_ratio(R1, A0, B0, theta):   \n",
    "    dA1 = R1 - A0\n",
    "    dB1 = R1**2 - B0\n",
    "    \n",
    "    A1 = A0 + theta*dA1\n",
    "    B1 = B0 + theta*dB1\n",
    "    \n",
    "    if ((B0 - A0**2) != 0):\n",
    "        D1 = (B0*dA1 - 0.5*A0*dB1) / ((B0 - A0**2)**(3/2))\n",
    "    else:\n",
    "        D1 = 0\n",
    "    \n",
    "    return D1, A1, B1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация алгоритма глубокого Q-обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фиксируем seed генераторов случайных чисел\n",
    "np.random.seed(1234)\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "#####################################################################\n",
    "#################### Начало оцениваемого задания ####################\n",
    "#####################################################################\n",
    "\n",
    "# Cоздайте Q-сеть со следующими параметрами:\n",
    "# Количество предиктивных признаков features_num = len(Features.columns)\n",
    "# Количество доступных активов assets_num = len(Actions.columns)\n",
    "# Количество рекуррентных ячеек в Q-сети rnn_layers = rnn_layers\n",
    "# Ширина скрытого слоя Q-сети n_hidden = n_hidden\n",
    "# Количество допустимых действий actions_num = len(Actions.index)\n",
    "\n",
    "Q_network = None\n",
    "\n",
    "#####################################################################\n",
    "#################### Конец оцениваемого задания #####################\n",
    "#####################################################################\n",
    "\n",
    "# Проводим обучение Q-сети на 10 эпизодах\n",
    "print(\"Начало обучения.\\n\")\n",
    "iter_num = 0\n",
    "for episode in range(1, 11):\n",
    "    \n",
    "    # Начальная конфигурация портфеля: 100% безрисковый актив\n",
    "    last_state = Actions.iloc[3]\n",
    "    A0, B0 = risk_free, risk_free**2\n",
    "    \n",
    "    # Обнуляем накопленную погрешность обучения Q-сети\n",
    "    loss = 0\n",
    "    \n",
    "    # Итерация по батчам в каждом эпизоде\n",
    "    for i in range(0, train_samples_num - 1, batch_size):\n",
    "        \n",
    "        # Массивы для хранения совершенных действий и целевых Q-значений\n",
    "        a_list, target_list = [], []\n",
    "        \n",
    "        # Обновление показателя ? в процедуре идентификации\n",
    "        iter_num += 1\n",
    "        Epsilon = Epsilon_start + (Epsilon_end - Epsilon_start) * min(iter_num / Epsilon_decay_periods, 1)\n",
    "        \n",
    "        # Итерация по наблюдениям в каждом батче\n",
    "        for j in range(i, min(i + batch_size, train_samples_num - 1)):\n",
    "            \n",
    "            # Предсказание сегодняшних Q-значений с помощью текущей версии Q-сети\n",
    "            Q_targets = Q_network.predict({'Factors': Features.iloc[j].values.reshape((1, 1, len(Features.iloc[j]))), \n",
    "                                           'Portfolio_Structure': last_state.values.reshape((1, len(last_state)))})[0]\n",
    "            \n",
    "            # Реализация ?-greedy процедуры идентификации\n",
    "            if (np.random.random() < Epsilon):\n",
    "                \n",
    "                #####################################################################\n",
    "                #################### Начало оцениваемого задания ####################\n",
    "                #####################################################################\n",
    "                \n",
    "                # Реализуйте выбор случайного действия из массива Actions. Используйте функцию np.random.choice.\n",
    "                # Сохраните индекс данного действия в массиве Actions, а также само действие (конфигурацию портфеля)\n",
    "                \n",
    "                a_index = None\n",
    "                a = None\n",
    "                \n",
    "                #####################################################################\n",
    "                #################### Конец оцениваемого задания #####################\n",
    "                #####################################################################\n",
    "                                \n",
    "            else:\n",
    "                a_index = np.argmax(Q_targets)\n",
    "                a = Actions.loc[a_index]\n",
    "            \n",
    "            # Вычисление размера брокерской комиссии за ребалансировку портфеля\n",
    "            delta_weights = abs(a - last_state)\n",
    "            transaction_costs = transaction_cost * delta_weights \n",
    "            \n",
    "            # Вычисление доходности финансовых активов в портфеле\n",
    "            portfolio_returns = np.multiply(a, Returns.iloc[j])\n",
    "            \n",
    "            # Вычисление чистой доходности портфеля\n",
    "            r = np.sum(portfolio_returns - transaction_costs)\n",
    "\n",
    "            # Вычисление дифференциального коэффициента Шарпа\n",
    "            D1, A0, B0 = differential_sharpe_ratio(r, A0, B0, theta)   \n",
    "\n",
    "            # Предсказание завтрашних Q-значений с помощью текущей версии Q-сети\n",
    "            Q1 = Q_network.predict({'Factors': Features.iloc[j+1].values.reshape((1, 1, len(Features.iloc[j+1]))), \n",
    "                                    'Portfolio_Structure': a.values.reshape((1, len(a)))})[0]\n",
    "\n",
    "            # Оценка погрешности обучения Q-сети\n",
    "            loss += abs(D1 + gamma * np.max(Q1) - Q_targets[a_index])\n",
    "            \n",
    "            #####################################################################\n",
    "            #################### Начало оцениваемого задания ####################\n",
    "            #####################################################################\n",
    "\n",
    "            # Вычислите целевые Q-значения на основе уравнения обновления.\n",
    "            # Используйте переменную D1 в качестве текущей награды.\n",
    "            # Не забудьте учесть параметр скорости обучения.\n",
    "            \n",
    "            Q_targets[a_index] = None\n",
    "\n",
    "            #####################################################################\n",
    "            #################### Конец оцениваемого задания #####################\n",
    "            #####################################################################\n",
    "            \n",
    "            # Сохранение совершенных действий и целевых Q-значений\n",
    "            a_list.append(list(last_state))\n",
    "            target_list.append(Q_targets)            \n",
    "            \n",
    "            # Сохранение последней конфигурации портфеля\n",
    "            last_state = a.copy()\n",
    "\n",
    "        # Сопоставление предиктивных факторов с полученным батчем наблюдений\n",
    "        train_features = Features.iloc[i : min(i + batch_size, train_samples_num - 1)]\n",
    "        train_features = train_features.values.reshape((train_features.shape[0], 1, train_features.shape[1]))\n",
    "        \n",
    "        # Дообучение Q-сети на полученном батче наблюдений\n",
    "        Q_network.train_on_batch(x = {'Factors': train_features,\n",
    "                                      'Portfolio_Structure': np.array(a_list)},\n",
    "                                 y = np.array(target_list))\n",
    "        \n",
    "    print ('Эпизод ' + str(episode) + '. Погрешность обучения Q-cети %0.3f.' % (loss / (train_samples_num - 1)))\n",
    "\n",
    "print(\"\\nОбучение на заданном количестве эпизодов завершено\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ эффективности AI агента на тестовом периоде"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка предобученной модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку качественное обучение Q-сети занимает большое количество времени, в дальнейшем анализе мы воспользуемся заранее предобученной моделью на большом количестве эпизодов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_network.load_weights(\"model.h5\")\n",
    "print (\"Предобученная модель успешно загружена\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Получение действий AI агента и вычисление доходностей управляемого портфеля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action_tracker = []\n",
    "test_portfolio_r = []\n",
    "\n",
    "# Начальная конфигурация портфеля: 100% безрисковый актив\n",
    "last_state = Actions.iloc[3]\n",
    "\n",
    "# Итерация по каждому дню тестового периода\n",
    "for i in range(train_samples_num, len(Features.index)):\n",
    "    \n",
    "    # Предсказание Q-значений с помощью обученной Q-сети\n",
    "    Q_targets = Q_network.predict({'Factors': Features.iloc[i].values.reshape((1, 1, len(Features.iloc[i]))), \n",
    "                                   'Portfolio_Structure': last_state.values.reshape((1, len(last_state)))})[0]\n",
    "    \n",
    "    # Выбор наилучшей конфигурации портфеля\n",
    "    a_index = np.argmax(Q_targets)\n",
    "    a = Actions.loc[a_index]\n",
    "    \n",
    "    # Сохранение выбранной конфигурации портфеля\n",
    "    action_tracker.append(a)\n",
    "    \n",
    "    # Вычисление размера брокерской комиссии за ребалансировку портфеля\n",
    "    delta_weights = abs(a - last_state)\n",
    "    transaction_costs = transaction_cost * delta_weights \n",
    "    \n",
    "    # Вычисление доходности финансовых активов в портфеле\n",
    "    portfolio_returns = np.multiply(a, Returns.iloc[i])\n",
    "       \n",
    "    # Сохранение чистой дневной доходности портфеля\n",
    "    test_portfolio_r.append(np.sum(portfolio_returns - transaction_costs))\n",
    "    \n",
    "test_portfolio_r = pd.Series(test_portfolio_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Анализ периода владения активами в процессе управления портфелем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку AI агент стремится максимизировать риск-взвешенную доходность портфеля, то он осуществляет его ребалансировку только в том случае, если наблюдается достаточно сильный сигнал продолжительного роста стоимости финансовых активов. В противном случае, AI агент предпочитает держать капитал в безрисковом активе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(action_tracker).rename('Период владения, %').reset_index().groupby(['index']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание бенчмарка и вычисление его доходностей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве бенчмарка рассматривается пассивная инвестиция в 4 доступных актива в равных пропорциях. Цель создания бенчмарка - проверить, сможет ли AI агент показать результаты лучше, чем простая альтернативная инвестиционная стратегия, доступная каждому инвестору."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_benchmark_value = pd.Series([0.25, 0.25, 0.25, 0.25], index = Returns.columns)\n",
    "test_benchmark_r = []\n",
    "\n",
    "# Итерация по каждому дню тестового периода\n",
    "for i in range(train_samples_num, len(Features.index)):\n",
    "\n",
    "    # Вычисление дневной доходности бенчмарка\n",
    "    r_b = np.sum(test_benchmark_value * (1.0 + Returns.iloc[i])) / np.sum(test_benchmark_value) - 1.0\n",
    "    \n",
    "    # Вычисление результирующей стоимости бенчмарка\n",
    "    test_benchmark_value = test_benchmark_value * (1.0 + Returns.iloc[i])\n",
    "        \n",
    "    # Сохранение дневной доходности бенчмарка\n",
    "    test_benchmark_r.append(r_b)\n",
    "\n",
    "test_benchmark_r = pd.Series(test_benchmark_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### График дневых доходностей портфеля против бенчмарка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.plot(Returns.iloc[train_samples_num:].index, test_portfolio_r, label = 'Управляемый портфель')\n",
    "ax.plot(Returns.iloc[train_samples_num:].index, test_benchmark_r, label = 'Бенчмарк')\n",
    "ax.legend()\n",
    "plt.title('График дневной доходности')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Анализ финансового результата управления портфелем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Доходность управляемого портфеля оказалась сопоставима с бенчмарком. Однако AI агент сумел добиться меньшей волатильности портфеля, в результате чего он получил значительно более высокий Коэффициент Шарпа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычисление доходностей портфелей\n",
    "portfolio_total_return = np.prod(1.0 + test_portfolio_r) - 1.0\n",
    "benchmark_total_return = np.prod(1.0 + test_benchmark_r) - 1.0\n",
    "\n",
    "# Вычисление стандартных отклонений портфелей\n",
    "portfolio_std = np.std(test_portfolio_r) * np.sqrt(len(test_portfolio_r))\n",
    "benchmark_std = np.std(test_benchmark_r) * np.sqrt(len(test_benchmark_r))\n",
    "\n",
    "# Вычисление коэффициентов Шарпа портфелей\n",
    "portfolio_sharpe_ratio = (portfolio_total_return - risk_free) / portfolio_std\n",
    "benchmark_sharpe_ratio = (benchmark_total_return - risk_free) / benchmark_std\n",
    "\n",
    "# Вывод таблицы с результатами\n",
    "print(tabulate([['Доходность', '%0.1f%%' % (100*benchmark_total_return), '%0.1f%%' % (100*portfolio_total_return)],\n",
    "                ['Ст. отклонение', '%0.1f%%' % (100*benchmark_std), '%0.1f%%' % (100*portfolio_std)],\n",
    "                ['Коэффициент Шарпа', '%0.2f' % benchmark_sharpe_ratio, '%0.2f' % portfolio_sharpe_ratio]],\n",
    "               headers = ['', 'Бенчмарк', 'Управляемый портфель']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
